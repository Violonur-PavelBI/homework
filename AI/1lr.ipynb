{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1\n",
    "### Конечномерная оптимизация, градиентные методы\n",
    "## Постановка задачи\n",
    "1) Запрогать GD модификацию, выбрать персонально 2 функции из книжки или scikit.\n",
    "\n",
    "2) Их визуализировать, реализовать sgd, sgd+LR schedule. \n",
    "\n",
    "3) Запрогать 1 моментный метод и 1 адаптивный. \n",
    "\n",
    "ДОП БАЛЛ: запрогать анимацию того как метод шагает по точкам на ландшафте (гифку например).\n",
    "\n",
    "Два варианта: выбрать 100-1000 случ точек из функции и исп в качестве датасета или сгенерировать блобы "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Mapping\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn as skl\n",
    "from sklearn import datasets\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython import display\n",
    "from numpy.random import permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#набор точек\n",
    "x, y = skl.datasets.make_blobs(n_samples=2, centers=None, n_features=1, random_state=0)\n",
    "# приведение к одной размерности\n",
    "x=np.reshape(x,y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Фунции и классы для добавления стохастичности оптимизатору ненужные в данном случае"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_sample(xs, ys):\n",
    "    \"\"\"\n",
    "        stochastic_sample: sample with replacement one x and one y\n",
    "        xs: all point on the plane\n",
    "        ys: all response on the plane\n",
    "        \n",
    "        return the one randomly selected x and y point\n",
    "    \"\"\"\n",
    "    perm = permutation(len(xs))\n",
    "    x = xs[perm[0]]\n",
    "    y = ys[perm[0]]\n",
    "    return x, y\n",
    "\n",
    "class lenDataloader():\n",
    "    '''бесмысленый класс для создания случайного бача из пар точек x,y созданный пока я забыл что эти точки и есть оптимизационные параметры'''\n",
    "    # инициатия двухмерного датасета\n",
    "    def __init__(self,x,y,batch_size: int = 1,shuffle: bool = True,drop_last: bool = False) -> None:\n",
    "        self.X=x\n",
    "        self.Y=y\n",
    "        self.batch_size=batch_size\n",
    "        self.drop_last=drop_last\n",
    "        self.shuffle=shuffle\n",
    "        self.batchindex=0\n",
    "        self.endbatchindex=0\n",
    "        self.X1=[]\n",
    "        self.Y1=[]\n",
    "    # отображение длины датасеета \n",
    "    def len(self) -> int:\n",
    "        return len(self.X)\n",
    "    # перемешивания индексов датасета если включено перемешивание иначе копируем\n",
    "    def permutation(self):\n",
    "        if (self.shuffle==True):\n",
    "            perm = permutation(self.len)\n",
    "            self.X1=[self.X[ind] for ind in perm]\n",
    "            self.Y1=[self.Y[ind] for ind in perm]\n",
    "        else:\n",
    "            self.X1=self.X\n",
    "            self.Y1=self.Y\n",
    "    def ReturnPermDataset(self,indxstart:int,indxstop=None,indxstep=1):\n",
    "        if indxstop==None:\n",
    "            return self.X1[indxstart],self.Y1[indxstart]\n",
    "        else:\n",
    "            return [self.X1[indxstart:indxstop:indxstep],self.Y1[indxstart:indxstop:indxstep]]\n",
    "    \n",
    "    def returnBatchSet(self):\n",
    "        if (self.batchindex==0):\n",
    "            # инициализация доп сета для перемешивания \n",
    "            self.permutation()\n",
    "            self.endbatchindex=(self.len() // self.batch_size)-1\n",
    "            # если датасет не делиться на ровные батчи отбрасываем последний по флагу\n",
    "            if ((self.drop_last==True) and (self.len() % self.batch_size !=0)):\n",
    "                self.endbatchindex-=1\n",
    "        \n",
    "        if ((self.drop_last==False) and (self.len() % self.batch_size !=0)and (self.batchindex+1==self.endbatchindex)):\n",
    "            batch[self.ReturnPermDataset(self.batchindex*self.batch_size, self.len())]\n",
    "        else:\n",
    "            batch=[self.ReturnPermDataset(self.batchindex*self.batch_size, self.batchindex*self.batch_size+self.batch_size)]\n",
    "        self.batchindex+=1\n",
    "        \n",
    "        return batch[0],batch[1]\n",
    "    def endEpoch(self):\n",
    "        self.batchindex=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Химмельблау\n",
    "$$ f(x, y) = (x^2 + y - 11)^2 + (x + y^2 - 7)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, array([-24,  40]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#функция 1\n",
    "def Himmelblau(x: np.float32, y: np.float32) -> np.float64:\n",
    "    '''\n",
    "    Функция Химмельблау\n",
    "    \n",
    "    Args:\n",
    "        x(np.ndarray): Аргумент 1\n",
    "        y(np.ndarray): Аргумент 2\n",
    "        \n",
    "    Returns:\n",
    "        np.float64: Результат функции\n",
    "    '''\n",
    "    return (np.square(np.square(x)+y-11)+np.square(x+np.square(y)-7));\n",
    "\n",
    "#градиент 1\n",
    "def Himmelblau_Grad(x: np.ndarray, y: np.ndarray) -> np.float64:\n",
    "    return np.array([(4*x*(np.square(x)+y-11)+2*(x+np.square(y)-7)), (2*(np.square(x)+y-11)+4*y*(x+np.square(y)-7))]);\n",
    "\n",
    "Himmelblau(2, 3), Himmelblau_Grad(2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Самая простейшая функция для оптимизации по 2 параметрам\n",
    "$$ f(x, y) = x^2  +  y^2  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pacha\\AppData\\Local\\Temp\\ipykernel_17716\\1950936567.py:1: DeprecationWarning: The symbol module is deprecated and will be removed in future versions of Python\n",
      "  from symbol import yield_stmt\n"
     ]
    }
   ],
   "source": [
    "from symbol import yield_stmt\n",
    "\n",
    "\n",
    "def mu_func(x: np.float32, y: np.float32) -> np.float64:\n",
    "    '''\n",
    "    Функция Химмельблау\n",
    "    \n",
    "    Args:\n",
    "        x(np.ndarray): Аргумент 1\n",
    "        y(np.ndarray): Аргумент 2\n",
    "        \n",
    "    Returns:\n",
    "        np.float64: Результат функции\n",
    "    '''\n",
    "    return x**2+y**2;\n",
    "\n",
    "#градиент 1\n",
    "def mu_func_Grad(x: np.ndarray, y: np.ndarray) -> np.float64:\n",
    "    return np.array([2*x, 2*y]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39ec22",
   "metadata": {},
   "source": [
    "## Функция Комрика\n",
    "$$ f(x, y) = sin(x + y)+(x - y)^2 - 1.5x + 2.5y +1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.186123335169764, array([-2.37252157,  5.22747843]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#функция 1\n",
    "def McCormick(x: np.float32, y: np.float32) -> np.float64:\n",
    "    '''\n",
    "    Функция Комрика\n",
    "    \n",
    "    Args:\n",
    "        x(np.ndarray): Аргумент 1\n",
    "        y(np.ndarray): Аргумент 2с\n",
    "        \n",
    "    Returns:\n",
    "        np.float64: Результат функции\n",
    "    '''\n",
    "    return np.sin(x + y)+np.square(x - y)-1.5*x+2.5*y+1;\n",
    "\n",
    "#градиент 1\n",
    "def McCormick_Grad(x: np.ndarray, y: np.ndarray) -> np.float64:\n",
    "    return np.array([np.cos(x+y)+2*(x-y)-1.5, np.cos(x+y)-2*(x-y)+2.5]);\n",
    "\n",
    "McCormick(2.5, 3.4), McCormick_Grad(2.5, 3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск (+ Learning Rate Schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_GD_LRS(f: Mapping, df: Mapping, x: np.ndarray, y: np.ndarray, lr: float = 0.01,\n",
    "          Epoch: int = 100, lre: int = 7, g: float = 0.1) -> Tuple [np.ndarray, np.ndarray, np.float32]:\n",
    "    '''\n",
    "    Моя простейшая реализация градиентного спуска + Learning Rate Schedule.\n",
    "    \n",
    "    Args:\n",
    "        f (Mapping): Функционал для оптимизации\n",
    "        df (Mapping): Градиент оптимизирующего функционала\n",
    "        x0 (np.ndarray): Стартовая точка 1\n",
    "        y0 (np.ndarray): Стартовая точка 2\n",
    "        lr (float): Скорость обучения. Default=0,01.\n",
    "        T (int): Количество итераций.\n",
    "        lre (int): Через сколько эпох мы уменьшим lr. Default = 7.\n",
    "        g (float): Коэфициент уменьшения lr [0,1). Default = 0.1\n",
    "    \n",
    "    Returns:\n",
    "        Tuple [np.ndarray, np.float32]: (x_optimal, f(x_optimal)).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    for i in range(Epoch):\n",
    "        if ((i%lre)==0):\n",
    "            lr *= g\n",
    "        n=df(x, y)\n",
    "        x = x - lr*n[0]\n",
    "        y = y - lr*n[1]\n",
    "        \n",
    "    return x, y, f(x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.80511807,  2.99997802]),\n",
       " array([3.13131252, 2.00005305]),\n",
       " array([9.70193205e-15, 4.23992343e-08]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_GD_LRS(Himmelblau,Himmelblau_Grad,x,y,lr=0.1,lre=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD+LRS + Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_GD_LRS_mom(f: Mapping, df: Mapping, x: np.ndarray, y: np.ndarray, lr: float = 0.1,\n",
    "          Epoch: int = 100, lre: int = 7, g: float = 0.1, momentum: float = 0.9) -> Tuple [np.ndarray, np.ndarray, np.float32]:\n",
    "    '''\n",
    "    Моя простейшая реализация градиентного спуска + Learning Rate Schedule + Momentum.\n",
    "    \n",
    "    Args:\n",
    "        f (Mapping): Функционал для оптимизации\n",
    "        df (Mapping): Градиент оптимизирующего функционала\n",
    "        x0 (np.ndarray): Стартовая точка 1\n",
    "        y0 (np.ndarray): Стартовая точка 2\n",
    "        lr (float): Скорость обучения. Default=0,01.\n",
    "        T (int): Количество итераций.\n",
    "        lre (int): Через сколько эпох мы уменьшим lr. Default = 7.\n",
    "        g (float): Коэфициент уменьшения lr [0,1). Default = 0.1\n",
    "        momentum(float): Коэфициент сохранения момента Default = 0.9\n",
    "    \n",
    "    Returns:\n",
    "        Tuple [np.ndarray, np.float32]: (x_optimal, f(x_optimal)).\n",
    "    \n",
    "    '''\n",
    "    n=np.zeros(df(x, y).shape)\n",
    "    for i in range(Epoch):\n",
    "        if ((i%lre)==0):\n",
    "            lr *= g\n",
    "        n=momentum*n-lr*(1-momentum)*df(x, y)\n",
    "        x = x + n[0]\n",
    "        y = y + n[1]\n",
    "    return x, y, f(x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.39390117, -2.89573665]),\n",
       " array([2.92455126, 3.024034  ]),\n",
       " array([6.20466678, 0.7314802 ]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_GD_LRS_mom(Himmelblau,Himmelblau_Grad,x,y,lr=0.1,lre=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GD+LRS + Nestorev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_GD_LRS_nest(f: Mapping, df: Mapping, x0: np.ndarray, y0: np.ndarray, lr: float = 0.01,\n",
    "          Epoch: int = 100, lre: int = 7, g: float = 0.1, Nestorev: float = 0.9) -> Tuple [np.ndarray, np.ndarray, np.float32]:\n",
    "    '''\n",
    "    Моя простейшая реализация градиентного спуска + Learning Rate Schedule + Nestorev.\n",
    "    \n",
    "    Args:\n",
    "        f (Mapping): Функционал для оптимизации\n",
    "        df (Mapping): Градиент оптимизирующего функционала\n",
    "        x0 (np.ndarray): Стартовая точка 1\n",
    "        y0 (np.ndarray): Стартовая точка 2\n",
    "        lr (float): Скорость обучения. Default=0,01.\n",
    "        T (int): Количество итераций.\n",
    "        lre (int): Через сколько эпох мы уменьшим lr. Default = 7.\n",
    "        g (float): Коэфициент уменьшения lr [0,1). Default = 0.1\n",
    "        Nestorev(float): Коэфициент сохранения момента памяти для последующего шага Default = 0.9\n",
    "    \n",
    "    Returns:\n",
    "        Tuple [np.ndarray, np.float32]: (x_optimal, f(x_optimal)).\n",
    "    \n",
    "    '''\n",
    "    x = x0;\n",
    "    y = y0;\n",
    "    n=np.zeros(df(x, y).shape)\n",
    "    for i in range(Epoch):\n",
    "        if ((i%lre)==0):\n",
    "            lr *= g\n",
    "        n=Nestorev*n-lr*(1-Nestorev)*df(x+Nestorev*n[0]-lr*(1-Nestorev)*df(x,y)[0], y+Nestorev*n[1]-lr*(1-Nestorev)*df(x,y)[1])\n",
    "        x = x + n[0]\n",
    "        y = y + n[1]\n",
    "        \n",
    "    return x, y, f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-2.62756952,  2.98555422]),\n",
       " array([3.12058643, 2.24287351]),\n",
       " array([0.96340252, 1.05679203]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_GD_LRS_nest(Himmelblau,Himmelblau_Grad,x,y,lr=0.1,lre=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adgard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_Adgard(f: Mapping, df: Mapping, x: np.ndarray, y: np.ndarray, lr: float = 0.01,\n",
    "          Epoch: int = 100, lre: int = 7, g: float = 0.1) -> Tuple [np.ndarray, np.ndarray, np.float32]:\n",
    "    '''\n",
    "    Моя простейшая реализация градиентного спуска + Learning Rate Schedule + Adgard.\n",
    "    \n",
    "    Args:\n",
    "        f (Mapping): Функционал для оптимизации\n",
    "        df (Mapping): Градиент оптимизирующего функционала\n",
    "        x0 (np.ndarray): Стартовая точка 1\n",
    "        y0 (np.ndarray): Стартовая точка 2\n",
    "        lr (float): Скорость обучения. Default=0,01.\n",
    "        T (int): Количество итераций.\n",
    "        lre (int): Через сколько эпох мы уменьшим lr. Default = 7.\n",
    "        g (float): Коэфициент уменьшения lr [0,1). Default = 0.1\n",
    "    \n",
    "    Returns:\n",
    "        Tuple [np.ndarray, np.float32]: (x_optimal, f(x_optimal)).\n",
    "    \n",
    "    '''\n",
    "\n",
    "    for i in range(Epoch):\n",
    "        if ((i%lre)==0):\n",
    "            lr *= g\n",
    "        \n",
    "        Gadapt=df(x, y)**2\n",
    "        n=lr*df(x, y)/(Gadapt+0.0000001)**0.5\n",
    "        x = x - n[0]\n",
    "        y = y - n[1]\n",
    "        \n",
    "    return x, y, f(x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.51427812,  5.41511271]),\n",
       " array([0.22222, 0.77778]),\n",
       " array([143.64531382, 365.81711471]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Adgard(Himmelblau,Himmelblau_Grad,x,y,lr=0.1,lre=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import M\n",
    "\n",
    "\n",
    "def my_Adam(f: Mapping, df: Mapping, x: np.ndarray, y: np.ndarray, lr: float = 0.01,\n",
    "          Epoch: int = 100, lre: int = 7, g: float = 0.1, momentum: float = 0.9,momentumsglaj: float = 0.9) -> Tuple [np.ndarray, np.ndarray, np.float32]:\n",
    "    '''\n",
    "    Моя простейшая реализация градиентного спуска + Learning Rate Schedule + Momentum.\n",
    "    \n",
    "    Args:\n",
    "        f (Mapping): Функционал для оптимизации\n",
    "        df (Mapping): Градиент оптимизирующего функционала\n",
    "        x0 (np.ndarray): Стартовая точка 1\n",
    "        y0 (np.ndarray): Стартовая точка 2\n",
    "        lr (float): Скорость обучения. Default=0,01.\n",
    "        T (int): Количество итераций.\n",
    "        lre (int): Через сколько эпох мы уменьшим lr. Default = 7.\n",
    "        g (float): Коэфициент уменьшения lr [0,1). Default = 0.1\n",
    "        momentum (float): Коэфициент сохранения момента Default = 0.9\n",
    "        momentumsglaj (float): Коэфициент сохранения момента сглаживания Default = 0.9\n",
    "    \n",
    "    Returns:\n",
    "        Tuple [np.ndarray, np.float32]: (x_optimal, f(x_optimal)).\n",
    "    \n",
    "    '''\n",
    "    m=np.zeros(df(x, y).shape)\n",
    "    Gadapt=np.zeros((df(x, y)**2).shape)\n",
    "    for i in range(Epoch):\n",
    "        if ((i%lre)==0):\n",
    "            lr *= g\n",
    "        m=momentum*m-lr*(1-momentum)*df(x, y)\n",
    "        Gadapt=momentumsglaj*Gadapt+(1-momentumsglaj)*df(x, y)**2\n",
    "        # if(i<100):\n",
    "            # m=m/(1-momentum)\n",
    "            # Gadapt=Gadapt/(1-momentumsglaj)\n",
    "        n=(lr*df(x, y)*m)/(Gadapt+0.0000001)**0.5\n",
    "        # print(M,Gadapt,n)\n",
    "        x = x - n[0]\n",
    "        y = y - n[1]\n",
    "        \n",
    "    return x, y, f(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.28725926,  5.71271918]),\n",
       " array([0.00283712, 1.00646645]),\n",
       " array([155.91649523, 512.71851218]))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_Adam(Himmelblau,Himmelblau_Grad,x,y,lr=0.1,lre=50,momentumsglaj=0.8,momentum=0.999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
